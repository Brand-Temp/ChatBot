{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BTEM3257_COMP5046_Ass1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "source": [
        "# Readme\n",
        "\n",
        "### There seems to be an error when importing the personality models that causes the responses to go quite strange, it may be necessary to train up a model from scratch should this occur. \n",
        "To start chatting you need to execute the Tokenisation Function, Ensure Lowercase Function, and Remove Non-alphabetic function code found under 1.2. Then, you may skip straight to loading the Seq2Seq, and loading the word embeddings model. To do this you need to import gensim from the first code cell in 2.1, then run the code cells under 2.1.6, and the variable define in the code cell under 2.1.5-6. Then, execute the code cell in 2.2.1 and the first code cel under 2.2 which imports numpy and tensorflow. From there you are free to move to executing 2.2.2, and then executing 2.2.5 to load the sequence by executing the three code blocks. Now simple execute all cells in section 3, and the chatbot should run.  \n",
        "To train a whole chatbot from scratch, run the code cells in the order they are presented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import all the pydrive things\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# Download the things\n",
        "professional_id = '1WXpmL0tcfJB_nKaI42z-F3kWbOoObdLu'\n",
        "comic_id = '1PwHiO0iDi1QdAsD87dqwfr9MabGjKGcQ'\n",
        "friend_id = '1x9X3QeGWgrkkuYcM_Aiw3qb8orBvn-m3'\n",
        "professional_download = drive.CreateFile({'id':professional_id})\n",
        "professional_download.GetContentFile('qna_chitchat_the_professional.tsv')\n",
        "comic_download = drive.CreateFile({'id':comic_id})\n",
        "comic_download.GetContentFile('qna_chitchat_the_comic.tsv')\n",
        "friend_download = drive.CreateFile({'id':friend_id})\n",
        "friend_download.GetContentFile('qna_chitchat_the_friend.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "###Steps in preprocessing:\n",
        "1. Extraction of personality data and storage in dictionary\n",
        "2. Tokenise Question data\n",
        "3. Remove stopwords from Questions and Answers\n",
        "4. Ensure Questions are all lowercase\n",
        "5. Remove non-alphabetic characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mjJq77l6K7G",
        "colab_type": "text"
      },
      "source": [
        "#### Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Initialise dictionary for the data once preprocessed\n",
        "friend = pd.read_csv('qna_chitchat_the_friend.tsv', sep='\\t')\n",
        "friend = friend.dropna()\n",
        "comic = pd.read_csv('qna_chitchat_the_comic.tsv', sep='\\t')\n",
        "comic = comic.dropna()\n",
        "professional = pd.read_csv('qna_chitchat_the_professional.tsv', sep='\\t')\n",
        "professional = professional.dropna()\n",
        "\n",
        "personality_data = {\n",
        "    'friend': friend,\n",
        "    'comic': comic,\n",
        "    'professional': professional\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWeRitYV6QsA",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenisation Function\n",
        "Defines the function to tokenise data (used for both training and chatting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65rZ1t566Ux6",
        "colab_type": "code",
        "outputId": "267008a4-36c0-4ee9-dc2f-dfc5348b9fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenise(words):\n",
        "    return word_tokenize(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuhuRoil6ihS",
        "colab_type": "text"
      },
      "source": [
        "#### Stopword Removal Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0_o9ns76iES",
        "colab_type": "code",
        "outputId": "ea76da06-8b5e-4d83-bc79-04e9215f1923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    return [w for w in words if w not in stopwords.words()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EExFfTAe6pP_",
        "colab_type": "text"
      },
      "source": [
        "#### Ensure Lowercase Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfblwQch6sc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_lowercase(words):\n",
        "    return [w.lower() for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMCD5bWu6vi0",
        "colab_type": "text"
      },
      "source": [
        "#### Remove Non-alphabetic Characters Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAWCmNTt61F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def remove_nonalphabetic(words):\n",
        "    no_puncts = []\n",
        "    for w in words:\n",
        "        w = str(w)\n",
        "        for p in puncts:\n",
        "            if p in w:\n",
        "                w = w.replace(p, '')\n",
        "        if len(w) > 0:\n",
        "            no_puncts.append(w)\n",
        "    return no_puncts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1iyA4AgfpHr",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess Personality Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEeB3ofOftWN",
        "colab_type": "code",
        "outputId": "060eb64f-5991-4e40-e069-d32d5dc1c6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "responses = {'friend': set(), 'professional': set(), 'comic': set()}\n",
        "resp_dict = {'friend': [], 'professional': [], 'comic': []}\n",
        "personalities = {'friend':[], 'professional':[], 'comic':[]}\n",
        "max_lens = {'friend': 0, 'professional': 0, 'comic': 0}\n",
        "\n",
        "# break up data into question-answer pairs\n",
        "for key, value in personality_data.items():\n",
        "    qnas = []\n",
        "    for index, qna in value.iterrows():\n",
        "        print(key, ': ', '{:%}'.format(index/value.shape[0]), end='')\n",
        "        question = qna['Question']\n",
        "        answer = qna['Answer']\n",
        "        question = tokenise(question)\n",
        "        question = to_lowercase(question)\n",
        "        question = remove_nonalphabetic(question)\n",
        "        question = list(filter(None, question))\n",
        "        max_lens[key] = max(max_lens[key], len(question))\n",
        "        qnas.append((question, answer))\n",
        "        responses[key].add(answer)\n",
        "        print('\\r', end='')\n",
        "    print('Basic preprocessing of', key, 'personalty is complete.')\n",
        "    personalities[key] = list(qnas)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Basic preprocessing of friend personalty is complete.\n",
            "Basic preprocessing of comic personalty is complete.\n",
            "Basic preprocessing of professional personalty is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0SYxQPCygjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dictionaries of responses for each personality\n",
        "# these need to include the _B_ and _E_\n",
        "for key, value in responses.items():\n",
        "    responses[key].add('_B_')\n",
        "    responses[key].add('_E_')\n",
        "    responses[key].add('_P_')\n",
        "    responses[key].add('_U_')\n",
        "    resp_list = list(responses[key])\n",
        "    resp_list.sort()\n",
        "    num_dict = {n: i for i, n in enumerate(resp_list)}\n",
        "    resp_dict[key] = (resp_list, num_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "I chose to use FastText as the word embedding model for the ChatBot. This is because FastText is able to deal with words outside of the original corpus, and given the nature of a chatbot, it would be very necessary it is able to understand words that weren't in the training data. For instance, its not uncommon for users to extend the number of letters in words for emphasis, or make words up if they don't know how to properly describe a phenomenon. These are scenarios that can't be covered using a Word2Vec model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ",
        "colab_type": "code",
        "outputId": "54ad2a95-ce20-4a8f-d76c-4e8d96b74df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Op66omXKVHa",
        "colab_type": "text"
      },
      "source": [
        "The dataset chosen was the TED Script dataset mentioned in one of the earlier tutorials. This dataset was both large, and likely to contain a larger variety of word then say, a dataset of movie scripts, owing to the formal, technical style of the TED talk. \n",
        "As far as a chat bot is concerned, the more complex, technical, or uncommon words present mightn't be likely to come up - however I felt should they, it would be best to have as accurate a vector as possible. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLjf_pm9NiA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import all the pydrive things\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "source": [
        "The data is preprocessed in the same way as the personality data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "outputId": "b1712280-96af-4d8d-8b5a-761d8016ac67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import re\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "xml = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "xml = etree.parse(xml)\n",
        "text = ''.join(xml.xpath('//content/text()'))\n",
        "text = re.sub(r'\\([^)]*\\)', '', text.lower())\n",
        "sentences = sent_tokenize(text)\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    s = tokenise(sentence)\n",
        "    data.append(remove_nonalphabetic(s))\n",
        "print(data[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same'], ['in', 'six', 'months', 'they', 'went', 'from', 'maximum', 'revenue', 'and', 'they', 'were', 'gone'], ['gone'], ['to', 'me', 'the', 'irony', 'about', 'the', 'facit', 'story', 'is', 'hearing', 'about', 'the', 'facit', 'engineers', 'who', 'had', 'bought', 'cheap', 'small', 'electronic', 'calculators', 'in', 'japan', 'that', 'they', 'used', 'to', 'doublecheck', 'their', 'calculators'], ['facit', 'did', 'too', 'much', 'exploitation'], ['but', 'exploration', 'can', 'go', 'wild', 'too'], ['a', 'few', 'years', 'back', 'i', 'worked', 'closely', 'alongside', 'a', 'european', 'biotech', 'company'], ['let', 's', 'call', 'them', 'oncosearch'], ['the', 'company', 'was', 'brilliant'], ['they', 'had', 'applications', 'that', 'promised', 'to', 'diagnose', 'even', 'cure', 'certain', 'forms', 'of', 'blood', 'cancer'], ['every', 'day', 'was', 'about', 'creating', 'something', 'new'], ['they', 'were', 'extremely', 'innovative', 'and', 'the', 'mantra', 'was', 'when', 'we', 'only', 'get', 'it', 'right', 'or', 'even', 'we', 'want', 'it', 'perfect'], ['the', 'sad', 'thing', 'is', 'before', 'they', 'became', 'perfect', 'even', 'good', 'enough', 'they', 'became', 'obsolete'], ['oncosearch', 'did', 'too', 'much', 'exploration'], ['i', 'first', 'heard', 'about', 'exploration', 'and', 'exploitation', 'about', '15', 'years', 'ago', 'when', 'i', 'worked', 'as', 'a', 'visiting', 'scholar', 'at', 'stanford', 'university'], ['the', 'founder', 'of', 'the', 'idea', 'is', 'jim', 'march'], ['and', 'to', 'me', 'the', 'power', 'of', 'the', 'idea', 'is', 'its', 'practicality'], ['exploration'], ['exploration', 'is', 'about', 'coming', 'up', 'with', 'what', 's', 'new'], ['it', 's', 'about', 'search', 'it', 's', 'about', 'discovery', 'it', 's', 'about', 'new', 'products', 'it', 's', 'about', 'new', 'innovations'], ['it', 's', 'about', 'changing', 'our', 'frontiers'], ['our', 'heroes', 'are', 'people', 'who', 'have', 'done', 'exploration', 'madame', 'curie', 'picasso', 'neil', 'armstrong', 'sir', 'edmund', 'hillary', 'etc'], ['i', 'come', 'from', 'norway', 'all', 'our', 'heroes', 'are', 'explorers', 'and', 'they', 'deserve', 'to', 'be'], ['we', 'all', 'know', 'that', 'exploration', 'is', 'risky'], ['we', 'do', 'nt', 'know', 'the', 'answers', 'we', 'do', 'nt', 'know', 'if', 'we', 're', 'going', 'to', 'find', 'them', 'and', 'we', 'know', 'that', 'the', 'risks', 'are', 'high'], ['exploitation', 'is', 'the', 'opposite'], ['exploitation', 'is', 'taking', 'the', 'knowledge', 'we', 'have', 'and', 'making', 'good', 'better'], ['exploitation', 'is', 'about', 'making', 'our', 'trains', 'run', 'on', 'time'], ['it', 's', 'about', 'making', 'good', 'products', 'faster', 'and', 'cheaper'], ['exploitation', 'is', 'not', 'risky', 'in', 'the', 'short', 'term'], ['but', 'if', 'we', 'only', 'exploit', 'it', 's', 'very', 'risky', 'in', 'the', 'long', 'term'], ['and', 'i', 'think', 'we', 'all', 'have', 'memories', 'of', 'the', 'famous', 'pop', 'groups', 'who', 'keep', 'singing', 'the', 'same', 'songs', 'again', 'and', 'again', 'until', 'they', 'become', 'obsolete', 'or', 'even', 'pathetic'], ['that', 's', 'the', 'risk', 'of', 'exploitation'], ['so', 'if', 'we', 'take', 'a', 'longterm', 'perspective', 'we', 'explore'], ['if', 'we', 'take', 'a', 'shortterm', 'perspective', 'we', 'exploit'], ['small', 'children', 'they', 'explore', 'all', 'day'], ['all', 'day', 'it', 's', 'about', 'exploration'], ['as', 'we', 'grow', 'older', 'we', 'explore', 'less', 'because', 'we', 'have', 'more', 'knowledge', 'to', 'exploit', 'on'], ['the', 'same', 'goes', 'for', 'companies'], ['companies', 'become', 'by', 'nature', 'less', 'innovative', 'as', 'they', 'become', 'more', 'competent'], ['and', 'this', 'is', 'of', 'course', 'a', 'big', 'worry', 'to', 'ceos'], ['and', 'i', 'hear', 'very', 'often', 'questions', 'phrased', 'in', 'different', 'ways'], ['for', 'example', 'how', 'can', 'i', 'both', 'effectively', 'run', 'and', 'reinvent', 'my', 'company'], ['or', 'how', 'can', 'i', 'make', 'sure', 'that', 'our', 'company', 'changes', 'before', 'we', 'become', 'obsolete', 'or', 'are', 'hit', 'by', 'a', 'crisis'], ['so', 'doing', 'one', 'well', 'is', 'difficult'], ['doing', 'both', 'well', 'as', 'the', 'same', 'time', 'is', 'art', 'pushing', 'both', 'exploration', 'and', 'exploitation'], ['so', 'one', 'thing', 'we', 've', 'found', 'is', 'only', 'about', 'two', 'percent', 'of', 'companies', 'are', 'able', 'to', 'effectively', 'explore', 'and', 'exploit', 'at', 'the', 'same', 'time', 'in', 'parallel'], ['but', 'when', 'they', 'do', 'the', 'payoffs', 'are', 'huge'], ['so', 'we', 'have', 'lots', 'of', 'great', 'examples'], ['we', 'have', 'nestl', 'creating', 'nespresso', 'we', 'have', 'lego', 'going', 'into', 'animated', 'films', 'toyota', 'creating', 'the', 'hybrids', 'unilever', 'pushing', 'into', 'sustainability', 'there', 'are', 'lots', 'of', 'examples', 'and', 'the', 'benefits', 'are', 'huge'], ['why', 'is', 'balancing', 'so', 'difficult'], ['i', 'think', 'it', 's', 'difficult', 'because', 'there', 'are', 'so', 'many', 'traps', 'that', 'keep', 'us', 'where', 'we', 'are'], ['so', 'i', 'll', 'talk', 'about', 'two', 'but', 'there', 'are', 'many'], ['so', 'let', 's', 'talk', 'about', 'the', 'perpetual', 'search', 'trap'], ['we', 'discover', 'something', 'but', 'we', 'do', 'nt', 'have', 'the', 'patience', 'or', 'the', 'persistence', 'to', 'get', 'at', 'it', 'and', 'make', 'it', 'work'], ['so', 'instead', 'of', 'staying', 'with', 'it', 'we', 'create', 'something', 'new'], ['but', 'the', 'same', 'goes', 'for', 'that', 'then', 'we', 're', 'in', 'the', 'vicious', 'circle', 'of', 'actually', 'coming', 'up', 'with', 'ideas', 'but', 'being', 'frustrated'], ['oncosearch', 'was', 'a', 'good', 'example'], ['a', 'famous', 'example', 'is', 'of', 'course', 'xerox'], ['but', 'we', 'do', 'nt', 'only', 'see', 'this', 'in', 'companies'], ['we', 'see', 'this', 'in', 'the', 'public', 'sector', 'as', 'well'], ['we', 'all', 'know', 'that', 'any', 'kind', 'of', 'effective', 'reform', 'of', 'education', 'research', 'health', 'care', 'even', 'defense', 'takes', '10', '15', 'maybe', '20', 'years', 'to', 'work'], ['but', 'still', 'we', 'change', 'much', 'more', 'often'], ['we', 'really', 'do', 'nt', 'give', 'them', 'the', 'chance'], ['another', 'trap', 'is', 'the', 'success', 'trap'], ['facit', 'fell', 'into', 'the', 'success', 'trap'], ['they', 'literally', 'held', 'the', 'future', 'in', 'their', 'hands', 'but', 'they', 'could', 'nt', 'see', 'it'], ['they', 'were', 'simply', 'so', 'good', 'at', 'making', 'what', 'they', 'loved', 'doing', 'that', 'they', 'would', 'nt', 'change'], ['we', 'are', 'like', 'that', 'too'], ['when', 'we', 'know', 'something', 'well', 'it', 's', 'difficult', 'to', 'change'], ['bill', 'gates', 'has', 'said', 'success', 'is', 'a', 'lousy', 'teacher'], ['it', 'seduces', 'us', 'into', 'thinking', 'we', 'can', 'not', 'fail'], ['that', 's', 'the', 'challenge', 'with', 'success'], ['so', 'i', 'think', 'there', 'are', 'some', 'lessons', 'and', 'i', 'think', 'they', 'apply', 'to', 'us'], ['and', 'they', 'apply', 'to', 'our', 'companies'], ['the', 'first', 'lesson', 'is', 'get', 'ahead', 'of', 'the', 'crisis'], ['and', 'any', 'company', 'that', 's', 'able', 'to', 'innovate', 'is', 'actually', 'able', 'to', 'also', 'buy', 'an', 'insurance', 'in', 'the', 'future'], ['netflix', 'they', 'could', 'so', 'easily', 'have', 'been', 'content', 'with', 'earlier', 'generations', 'of', 'distribution', 'but', 'they', 'always', 'and', 'i', 'think', 'they', 'will', 'always', 'keep', 'pushing', 'for', 'the', 'next', 'battle'], ['i', 'see', 'other', 'companies', 'that', 'say', 'i', 'll', 'win', 'the', 'next', 'innovation', 'cycle', 'whatever', 'it', 'takes'], ['second', 'one', 'think', 'in', 'multiple', 'time', 'scales'], ['i', 'll', 'share', 'a', 'chart', 'with', 'you', 'and', 'i', 'think', 'it', 's', 'a', 'wonderful', 'one'], ['any', 'company', 'we', 'look', 'at', 'taking', 'a', 'oneyear', 'perspective', 'and', 'looking', 'at', 'the', 'valuation', 'of', 'the', 'company', 'innovation', 'typically', 'accounts', 'for', 'only', 'about', '30', 'percent'], ['so', 'when', 'we', 'think', 'one', 'year', 'innovation', 'is', 'nt', 'really', 'that', 'important'], ['move', 'ahead', 'take', 'a', '10year', 'perspective', 'on', 'the', 'same', 'company', 'suddenly', 'innovation', 'and', 'ability', 'to', 'renew', 'account', 'for', '70', 'percent'], ['but', 'companies', 'ca', 'nt', 'choose'], ['they', 'need', 'to', 'fund', 'the', 'journey', 'and', 'lead', 'the', 'long', 'term'], ['third', 'invite', 'talent'], ['i', 'do', 'nt', 'think', 'it', 's', 'possible', 'for', 'any', 'of', 'us', 'to', 'be', 'able', 'to', 'balance', 'exploration', 'and', 'exploitation', 'by', 'ourselves'], ['i', 'think', 'it', 's', 'a', 'team', 'sport'], ['i', 'think', 'we', 'need', 'to', 'allow', 'challenging'], ['i', 'think', 'the', 'mark', 'of', 'a', 'great', 'company', 'is', 'being', 'open', 'to', 'be', 'challenged', 'and', 'the', 'mark', 'of', 'a', 'good', 'corporate', 'board', 'is', 'to', 'constructively', 'challenge']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "source": [
        "We're using a CBOW model here. I felt through experimentation during the week 3 lab that the CBOW gave the more acurate sense of similar words when presented with invented words, and words outside of the vocabulary of the original data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings_model = FastText(\n",
        "    size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=5,\n",
        "    sg=0,\n",
        "    word_ngrams=1\n",
        ")\n",
        "word_embeddings_model.build_vocab(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings_model.train(sentences=data, total_examples=len(data), epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr4s-KgQKMu3",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5-6 Local or Cloud model Saving Variable\n",
        "\n",
        "Swap this variable to True if you wish to not load and save the word embeddings model from the cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znNrmj7VJgfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_load_embeddings_locally = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pickle\n",
        "\n",
        "def save_embedding(model):\n",
        "    name = 'word_embeddings_model_BTEM3257.model'\n",
        "    model.save(name)\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    if save_load_embeddings_locally is False:\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "\n",
        "        saved_word_embeddings_a = drive.CreateFile({'title': 'word_embeddings_model_BTEM3257.model'})\n",
        "        saved_word_embeddings_b = drive.CreateFile({'title': 'word_embeddings_model_BTEM3257.model.trainables.vectors_ngrams_lockf.npy'})\n",
        "        saved_word_embeddings_c = drive.CreateFile({'title': 'word_embeddings_model_BTEM3257.model.wv.vectors_ngrams.npy'})\n",
        "        \n",
        "        saved_resp_dict = drive.CreateFile({'title': 'resp_dict'})\n",
        "        with open('resp_dict.pkl', 'wb') as f:\n",
        "            pickle.dump(resp_dict, f)\n",
        "\n",
        "\n",
        "        saved_word_embeddings_a.SetContentFile('./word_embeddings_model_BTEM3257.model')\n",
        "        saved_word_embeddings_b.SetContentFile('./word_embeddings_model_BTEM3257.model.trainables.vectors_ngrams_lockf.npy')\n",
        "        saved_word_embeddings_c.SetContentFile('./word_embeddings_model_BTEM3257.model.wv.vectors_ngrams.npy')\n",
        "        saved_resp_dict.SetContentFile('./resp_dict.pkl')\n",
        "\n",
        "\n",
        "        saved_word_embeddings_a.Upload()\n",
        "        saved_word_embeddings_b.Upload()\n",
        "        saved_word_embeddings_c.Upload()\n",
        "        saved_resp_dict.Upload()\n",
        "\n",
        "\n",
        "        print('title: %s, id: %s' % (saved_word_embeddings_a['title'], saved_word_embeddings_a['id']))\n",
        "        print('title: %s, id: %s' % (saved_word_embeddings_b['title'], saved_word_embeddings_b['id']))\n",
        "        print('title: %s, id: %s' % (saved_word_embeddings_c['title'], saved_word_embeddings_c['id']))\n",
        "        print('title: %s, id: %s' % (saved_resp_dict['title'], saved_resp_dict['id']))            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTA28nhdZZb3",
        "colab_type": "code",
        "outputId": "f8e8bb8e-bda6-486e-940e-288561b362fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "save_embedding(word_embeddings_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: word_embeddings_model_BTEM3257.model, id: 1BE6tQBWx4dHSZVBV_75T-qAMzAOLaLoV\n",
            "title: word_embeddings_model_BTEM3257.model.trainables.vectors_ngrams_lockf.npy, id: 1rggfUJTNrNz8ph_oC9QXB7gNMfxHYEci\n",
            "title: word_embeddings_model_BTEM3257.model.wv.vectors_ngrams.npy, id: 1x6o6mMwZ6n2baXFwZySuqaaBAkZIs-hW\n",
            "title: resp_dict, id: 1o0_R2Z9zftaqBBAVFle7cKBhrF5mD11j\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWi2cFEfuC6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings_model_a_id = '1BE6tQBWx4dHSZVBV_75T-qAMzAOLaLoV'\n",
        "word_embeddings_model_b_id = '1rggfUJTNrNz8ph_oC9QXB7gNMfxHYEci'\n",
        "word_embeddings_model_c_id = '1x6o6mMwZ6n2baXFwZySuqaaBAkZIs-hW'\n",
        "resp_dict_id = '1o0_R2Z9zftaqBBAVFle7cKBhrF5mD11j'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pickle\n",
        "\n",
        "def load_embeddings():\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    if save_load_embeddings_locally is False:\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "    \n",
        "        word_embeddings_a = drive.CreateFile({'id': word_embeddings_model_a_id})\n",
        "        word_embeddings_b = drive.CreateFile({'id': word_embeddings_model_b_id})\n",
        "        word_embeddings_c = drive.CreateFile({'id': word_embeddings_model_c_id})\n",
        "        resp_dict_dl = drive.CreateFile({'id': resp_dict_id})\n",
        "\n",
        "    \n",
        "        word_embeddings_a.GetContentFile('word_embeddings_model_BTEM3257.model')\n",
        "        word_embeddings_b.GetContentFile('word_embeddings_model_BTEM3257.model.trainables.vectors_ngrams_lockf.npy')\n",
        "        word_embeddings_c.GetContentFile('word_embeddings_model_BTEM3257.model.wv.vectors_ngrams.npy')\n",
        "        resp_dict_dl.GetContentFile('resp_dict.pkl')\n",
        "        \n",
        "\n",
        "    model = FastText.load('word_embeddings_model_BTEM3257.model')\n",
        "    with open('resp_dict.pkl', 'rb') as f:\n",
        "        r_d = pickle.load(f)\n",
        "    \n",
        "    return model, r_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNKCM38OcjpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings, resp_dict = load_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XJmOmN6LXlM",
        "colab_type": "text"
      },
      "source": [
        "### Justification for Hyperparameters\n",
        "#### Learning rate 0.001:\n",
        "##### Adam with epsilon 0.1:\n",
        "Friend - 5002 with a final cost of  0.836100  \n",
        "Professional - Stopeed at epoch: 5003 with a final cost of  0.932553  \n",
        "Stopeed at epoch: 5003 with a final cost of  0.864433  \n",
        "\n",
        "##### Adam with default:\n",
        "all lower - between 0.001 and 0.004, but more irratic, harder to ensure the lowest loss\n",
        "\n",
        "##### Adam with epsilon 0.01:\n",
        "Friend - Stopeed at epoch: 5002 with a final cost of  0.014182  \n",
        "Professional - Stopeed at epoch: 5002 with a final cost of  0.015218  \n",
        "Comic - Stopeed at epoch: 5002 with a final cost of  0.014376  \n",
        "\n",
        "#### Learning Rate 0.01:\n",
        "##### epsilon 0.1:\n",
        "Friend - Stopeed at epoch: 5003 with a final cost of  0.011892  \n",
        "Professional - Stopeed at epoch: 5002 with a final cost of  0.013488  \n",
        "Comic - Stopeed at epoch: 5002 with a final cost of  0.011853\n",
        "\n",
        "##### epsilon 0.01\n",
        "Friend - Stopeed at epoch: 5005 with a final cost of  0.002911  \n",
        "Professional - Stopeed at epoch: 5002 with a final cost of  0.003078  \n",
        "Comic - Stopeed at epoch: 5002 with a final cost of  0.007007\n",
        "\n",
        "#####  epsilon default\n",
        "Friend - Stopeed at epoch: 5002 with a final cost of  1.052485  \n",
        "Professional - Stopeed at epoch: 5002 with a final cost of  1.634274  \n",
        "Stopeed at epoch: 5002 with a final cost of  1.311647\n",
        "\n",
        "##### epsilon 0.001\n",
        "Friend - Stopeed at epoch: 5004 with a final cost of  0.422949  \n",
        "Professional - Stopeed at epoch: 5003 with a final cost of  1.424263  \n",
        "Comic - Stopeed at epoch: 5003 with a final cost of  1.167939\n",
        "\n",
        "\n",
        "\n",
        "Given these test results I'm going to take a 0.01 Learning rate and a 0.01 epsilon and increase the epochs from 5000 to 7500, then 10000 to see the restuls\n",
        "#### 7500 epochs\n",
        "friend - Stopeed at epoch: 7504 with a final cost of  0.003122  \n",
        "professional - Stopeed at epoch: 7502 with a final cost of  0.003158  \n",
        "comic - Stopeed at epoch: 7503 with a final cost of  0.004225\n",
        "\n",
        "Since it was fairly clear from increasing the epochs that there wasn't a substantial reduction in cost (other than for the comic personality), I decided not to test on 10000 to avoid the risk of overfitting. \n",
        "\n",
        "This is how I came up with the minimum epoch of 5000, a learning rate of 0.01 and an 0.01 epsilon value for my models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoCKOXmt78nh",
        "colab_type": "code",
        "outputId": "6682f76c-3f9a-4b07-fd78-25b0da011750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "# Global Variables\n",
        "# Define Model Global Variables    \n",
        "learning_rate = 0.01\n",
        "number_hidden = 128\n",
        "\n",
        "personality_models = {}\n",
        "\n",
        "number_classes = 105\n",
        "input_size = 100\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "tf.reset_default_graph()\n",
        "encoder_input = tf.placeholder(tf.float32, [None, None, input_size])\n",
        "decoder_input = tf.placeholder(tf.float32, [None, None, 105])\n",
        "targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "# Define Encoder Cell\n",
        "with tf.variable_scope('encode'):\n",
        "    encoder_cell = tf.nn.rnn_cell.BasicRNNCell(number_hidden)\n",
        "    encoder_cell = tf.nn.rnn_cell.DropoutWrapper(encoder_cell, output_keep_prob=0.5)    \n",
        "    outputs, encoder_states = tf.nn.dynamic_rnn(encoder_cell, encoder_input, dtype=tf.float32)\n",
        "        \n",
        "# Define Decoder Cell\n",
        "with tf.variable_scope('decode'):\n",
        "    decoder_cell = tf.nn.rnn_cell.BasicRNNCell(number_hidden)\n",
        "    decoder_cell = tf.nn.rnn_cell.DropoutWrapper(decoder_cell, output_keep_prob=0.5)\n",
        "    outputs, decoder_states = tf.nn.dynamic_rnn(decoder_cell, decoder_input, initial_state=encoder_states, dtype=tf.float32)\n",
        "\n",
        "# Define Model\n",
        "model = tf.layers.dense(outputs, number_classes, activation=None)\n",
        "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
        "optimiser = tf.train.AdamOptimizer(learning_rate, epsilon=0.01).minimize(cost)\n",
        "   \n",
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-21-8e9c3f179aae>:18: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-21-8e9c3f179aae>:20: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-21-8e9c3f179aae>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99sEw4CC7yRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function Definitions\n",
        "def get_q_vector(question):\n",
        "    # need to pad this so they all have the same length\n",
        "    difference = max_len - len(question)\n",
        "    for _ in range(difference):\n",
        "        question.append('_P_')\n",
        "    vectors = []\n",
        "    for token in question:\n",
        "        vectors.append(word_embeddings[token])\n",
        "    return vectors\n",
        "    \n",
        "\n",
        "def get_a_vector(answer, r_i):\n",
        "    i = r_i[answer]\n",
        "    return i\n",
        "\n",
        "def make_batch(sequence_data, r_i):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "    \n",
        "    for seq in sequence_data:\n",
        "        input_data = get_q_vector(seq[0])\n",
        "        output_data = [get_a_vector('_B_', r_i), get_a_vector(seq[1], r_i)]\n",
        "        target_data = [get_a_vector(seq[1], r_i), get_a_vector('_E_', r_i)]\n",
        "        \n",
        "        input_batch.append(input_data)\n",
        "        output_batch.append(np.eye(105)[output_data])\n",
        "        target_batch.append(target_data)\n",
        "    \n",
        "    return input_batch, output_batch, target_batch\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pB15ZODZnn",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "outputId": "36ab9678-26b4-4f26-a6a5-b740908f10a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2965
        }
      },
      "source": [
        "total_epochs = 5000\n",
        "for key, value in personalities.items():\n",
        "    max_len = max_lens[key]\n",
        "    response_indexes = resp_dict[key][1]\n",
        "        \n",
        "    input_batch, output_batch, target_batch = make_batch(personalities[key], response_indexes)\n",
        "\n",
        "        \n",
        "    print('Now training ', key, ' model')\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    \n",
        "    curr_epoch = 0\n",
        "    prev_loss = 0\n",
        "    curr_loss = -1\n",
        "    \n",
        "    while True:\n",
        "        _, curr_loss = session.run([optimiser, cost],\n",
        "                             feed_dict={\n",
        "                                 encoder_input: input_batch,\n",
        "                                 decoder_input: output_batch,\n",
        "                                 targets: target_batch\n",
        "                             })\n",
        "        if curr_epoch > total_epochs:\n",
        "            if curr_loss > prev_loss:\n",
        "                print('Stopeed at epoch: %04d' % (curr_epoch+1), 'with a final cost of ', '{:.6f}'.format(curr_loss))\n",
        "                break\n",
        "        if curr_epoch % 100 == 0:\n",
        "            print('Epoch:', '%04d' % (curr_epoch + 1),\n",
        "            'cost =', '{:.6f}'.format(curr_loss))\n",
        "        prev_loss = curr_loss\n",
        "        curr_epoch += 1\n",
        "            \n",
        "    saver = tf.train.Saver()\n",
        "    save_path =  './' + key\n",
        "    saver.save(session, save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now training  friend  model\n",
            "Epoch: 0001 cost = 4.731266\n",
            "Epoch: 0101 cost = 2.246108\n",
            "Epoch: 0201 cost = 1.520328\n",
            "Epoch: 0301 cost = 0.510572\n",
            "Epoch: 0401 cost = 0.136607\n",
            "Epoch: 0501 cost = 0.062369\n",
            "Epoch: 0601 cost = 0.035651\n",
            "Epoch: 0701 cost = 0.024743\n",
            "Epoch: 0801 cost = 0.017703\n",
            "Epoch: 0901 cost = 0.015311\n",
            "Epoch: 1001 cost = 0.012084\n",
            "Epoch: 1101 cost = 0.011020\n",
            "Epoch: 1201 cost = 0.011098\n",
            "Epoch: 1301 cost = 0.008911\n",
            "Epoch: 1401 cost = 0.008126\n",
            "Epoch: 1501 cost = 0.006996\n",
            "Epoch: 1601 cost = 0.007357\n",
            "Epoch: 1701 cost = 0.006110\n",
            "Epoch: 1801 cost = 0.005293\n",
            "Epoch: 1901 cost = 0.005254\n",
            "Epoch: 2001 cost = 0.005044\n",
            "Epoch: 2101 cost = 0.004210\n",
            "Epoch: 2201 cost = 0.004328\n",
            "Epoch: 2301 cost = 0.005125\n",
            "Epoch: 2401 cost = 0.004316\n",
            "Epoch: 2501 cost = 0.005902\n",
            "Epoch: 2601 cost = 0.004451\n",
            "Epoch: 2701 cost = 0.005667\n",
            "Epoch: 2801 cost = 0.004035\n",
            "Epoch: 2901 cost = 0.003969\n",
            "Epoch: 3001 cost = 0.004172\n",
            "Epoch: 3101 cost = 0.003924\n",
            "Epoch: 3201 cost = 0.003825\n",
            "Epoch: 3301 cost = 0.003687\n",
            "Epoch: 3401 cost = 0.004401\n",
            "Epoch: 3501 cost = 0.003818\n",
            "Epoch: 3601 cost = 0.003283\n",
            "Epoch: 3701 cost = 0.003806\n",
            "Epoch: 3801 cost = 0.003261\n",
            "Epoch: 3901 cost = 0.003793\n",
            "Epoch: 4001 cost = 0.927540\n",
            "Epoch: 4101 cost = 0.053827\n",
            "Epoch: 4201 cost = 0.019184\n",
            "Epoch: 4301 cost = 0.015377\n",
            "Epoch: 4401 cost = 0.010649\n",
            "Epoch: 4501 cost = 0.009373\n",
            "Epoch: 4601 cost = 0.007727\n",
            "Epoch: 4701 cost = 0.007353\n",
            "Epoch: 4801 cost = 0.007477\n",
            "Epoch: 4901 cost = 0.006548\n",
            "Epoch: 5001 cost = 0.006378\n",
            "Stopeed at epoch: 5002 with a final cost of  0.006561\n",
            "Now training  professional  model\n",
            "Epoch: 0001 cost = 4.656511\n",
            "Epoch: 0101 cost = 2.237886\n",
            "Epoch: 0201 cost = 1.368794\n",
            "Epoch: 0301 cost = 0.361558\n",
            "Epoch: 0401 cost = 0.093731\n",
            "Epoch: 0501 cost = 0.044240\n",
            "Epoch: 0601 cost = 0.026295\n",
            "Epoch: 0701 cost = 0.019063\n",
            "Epoch: 0801 cost = 0.016651\n",
            "Epoch: 0901 cost = 0.013356\n",
            "Epoch: 1001 cost = 0.010548\n",
            "Epoch: 1101 cost = 0.009304\n",
            "Epoch: 1201 cost = 0.009020\n",
            "Epoch: 1301 cost = 0.008344\n",
            "Epoch: 1401 cost = 0.006976\n",
            "Epoch: 1501 cost = 0.007076\n",
            "Epoch: 1601 cost = 0.006268\n",
            "Epoch: 1701 cost = 0.007364\n",
            "Epoch: 1801 cost = 0.003626\n",
            "Epoch: 1901 cost = 0.005068\n",
            "Epoch: 2001 cost = 0.004882\n",
            "Epoch: 2101 cost = 0.005477\n",
            "Epoch: 2201 cost = 0.005561\n",
            "Epoch: 2301 cost = 0.004548\n",
            "Epoch: 2401 cost = 0.004213\n",
            "Epoch: 2501 cost = 0.003254\n",
            "Epoch: 2601 cost = 0.005395\n",
            "Epoch: 2701 cost = 0.005635\n",
            "Epoch: 2801 cost = 0.003643\n",
            "Epoch: 2901 cost = 0.004720\n",
            "Epoch: 3001 cost = 0.004230\n",
            "Epoch: 3101 cost = 0.003759\n",
            "Epoch: 3201 cost = 0.004126\n",
            "Epoch: 3301 cost = 0.004206\n",
            "Epoch: 3401 cost = 0.003297\n",
            "Epoch: 3501 cost = 0.003815\n",
            "Epoch: 3601 cost = 0.003375\n",
            "Epoch: 3701 cost = 0.003195\n",
            "Epoch: 3801 cost = 0.003449\n",
            "Epoch: 3901 cost = 0.002885\n",
            "Epoch: 4001 cost = 0.003832\n",
            "Epoch: 4101 cost = 0.003608\n",
            "Epoch: 4201 cost = 0.002804\n",
            "Epoch: 4301 cost = 0.003648\n",
            "Epoch: 4401 cost = 0.003058\n",
            "Epoch: 4501 cost = 0.003015\n",
            "Epoch: 4601 cost = 0.003348\n",
            "Epoch: 4701 cost = 0.003847\n",
            "Epoch: 4801 cost = 0.002964\n",
            "Epoch: 4901 cost = 0.003830\n",
            "Epoch: 5001 cost = 0.003509\n",
            "Stopeed at epoch: 5003 with a final cost of  0.003258\n",
            "Now training  comic  model\n",
            "Epoch: 0001 cost = 4.834768\n",
            "Epoch: 0101 cost = 2.267140\n",
            "Epoch: 0201 cost = 1.593010\n",
            "Epoch: 0301 cost = 0.497813\n",
            "Epoch: 0401 cost = 0.127317\n",
            "Epoch: 0501 cost = 0.048449\n",
            "Epoch: 0601 cost = 0.036972\n",
            "Epoch: 0701 cost = 0.023749\n",
            "Epoch: 0801 cost = 0.016016\n",
            "Epoch: 0901 cost = 0.014905\n",
            "Epoch: 1001 cost = 0.013030\n",
            "Epoch: 1101 cost = 0.009356\n",
            "Epoch: 1201 cost = 0.009568\n",
            "Epoch: 1301 cost = 0.009400\n",
            "Epoch: 1401 cost = 0.007131\n",
            "Epoch: 1501 cost = 0.007144\n",
            "Epoch: 1601 cost = 0.006810\n",
            "Epoch: 1701 cost = 0.006346\n",
            "Epoch: 1801 cost = 0.006888\n",
            "Epoch: 1901 cost = 0.006521\n",
            "Epoch: 2001 cost = 0.005655\n",
            "Epoch: 2101 cost = 0.005470\n",
            "Epoch: 2201 cost = 0.004747\n",
            "Epoch: 2301 cost = 0.348204\n",
            "Epoch: 2401 cost = 0.035660\n",
            "Epoch: 2501 cost = 0.017567\n",
            "Epoch: 2601 cost = 0.012200\n",
            "Epoch: 2701 cost = 0.010705\n",
            "Epoch: 2801 cost = 0.009056\n",
            "Epoch: 2901 cost = 0.008948\n",
            "Epoch: 3001 cost = 0.008128\n",
            "Epoch: 3101 cost = 0.007025\n",
            "Epoch: 3201 cost = 0.005673\n",
            "Epoch: 3301 cost = 0.004581\n",
            "Epoch: 3401 cost = 0.005015\n",
            "Epoch: 3501 cost = 0.004053\n",
            "Epoch: 3601 cost = 0.004306\n",
            "Epoch: 3701 cost = 0.005185\n",
            "Epoch: 3801 cost = 0.003353\n",
            "Epoch: 3901 cost = 0.004375\n",
            "Epoch: 4001 cost = 0.003862\n",
            "Epoch: 4101 cost = 0.004451\n",
            "Epoch: 4201 cost = 0.003924\n",
            "Epoch: 4301 cost = 0.003636\n",
            "Epoch: 4401 cost = 0.003988\n",
            "Epoch: 4501 cost = 0.003979\n",
            "Epoch: 4601 cost = 0.003578\n",
            "Epoch: 4701 cost = 0.003670\n",
            "Epoch: 4801 cost = 0.002934\n",
            "Epoch: 4901 cost = 0.003360\n",
            "Epoch: 5001 cost = 0.004117\n",
            "Stopeed at epoch: 5004 with a final cost of  0.003414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def save_sequence():\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    \n",
        "    saved_checkpoint = drive.CreateFile({'title':'checkpoint'})\n",
        "    saved_checkpoint.SetContentFile('./checkpoint')\n",
        "    saved_checkpoint.Upload()\n",
        "    print('title: %s, id: %s' % (saved_checkpoint['title'], saved_checkpoint['id']))\n",
        "    \n",
        "    for p in ['friend', 'comic', 'professional']:\n",
        "        saved_a = drive.CreateFile({'title': p+'.index'})\n",
        "        saved_b = drive.CreateFile({'title': p+'.meta'})    \n",
        "        saved_c = drive.CreateFile({'title': p+'.data-00000-of-00001'})    \n",
        "    \n",
        "        saved_a.SetContentFile('./'+p+'.index')\n",
        "        saved_b.SetContentFile('./'+p+'.meta')\n",
        "        saved_c.SetContentFile('./'+p+'.data-00000-of-00001')    \n",
        "    \n",
        "        saved_a.Upload()\n",
        "        saved_b.Upload()\n",
        "        saved_c.Upload()\n",
        "\n",
        "\n",
        "        print('title: %s, id: %s' % (saved_a['title'], saved_a['id']))\n",
        "        print('title: %s, id: %s' % (saved_b['title'], saved_b['id']))\n",
        "        print('title: %s, id: %s' % (saved_c['title'], saved_c['id']))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtZvcHDPvLZn",
        "colab_type": "code",
        "outputId": "e96a3e7d-553e-4aa1-f098-df6795285d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "save_sequence()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: checkpoint, id: 1VCSfWQyIlmOgkokkPqtEe-JzQ6LjyPr3\n",
            "title: friend.index, id: 1Z5_GsJABoejrYOVCHi3T8_KRpbfFM41c\n",
            "title: friend.meta, id: 18SyVsGalR0ApqSLVQVnVPd2QDpuA_v_D\n",
            "title: friend.data-00000-of-00001, id: 1H162ZArHs6mkaIYw0SbSIQryeWegO4lO\n",
            "title: comic.index, id: 17XpnG9vYUI067GW4pkZfiCanE_0sSVTU\n",
            "title: comic.meta, id: 1RJSR-qgxITc13UpacuObkTuMp0Vkc902\n",
            "title: comic.data-00000-of-00001, id: 1Uc7Jl1uOQkTtLNBBWZq5i46I5b-2B3NW\n",
            "title: professional.index, id: 1mh-aPFdWBYuTDB8DVEd-1a20L6FBuisb\n",
            "title: professional.meta, id: 1fFBplK8F8tdHexgrU9-lblYt3z_jHGXY\n",
            "title: professional.data-00000-of-00001, id: 1JlDnnKTymjzuFmgMjavHcsfrxiYAvTXu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ovm-xrPYIy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check = '1VCSfWQyIlmOgkokkPqtEe-JzQ6LjyPr3'\n",
        "a = ['1Z5_GsJABoejrYOVCHi3T8_KRpbfFM41c', '17XpnG9vYUI067GW4pkZfiCanE_0sSVTU', '1mh-aPFdWBYuTDB8DVEd-1a20L6FBuisb']\n",
        "b = ['18SyVsGalR0ApqSLVQVnVPd2QDpuA_v_D', '1RJSR-qgxITc13UpacuObkTuMp0Vkc902', '1fFBplK8F8tdHexgrU9-lblYt3z_jHGXY']\n",
        "c = ['1H162ZArHs6mkaIYw0SbSIQryeWegO4lO', '1Uc7Jl1uOQkTtLNBBWZq5i46I5b-2B3NW', '1JlDnnKTymjzuFmgMjavHcsfrxiYAvTXu']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def load_sequence():\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    \n",
        "    download_check = drive.CreateFile({'id': check})\n",
        "    download_check.GetContentFile('checkpoint')\n",
        "    \n",
        "    i = 0\n",
        "    for p in ['friend', 'comic', 'professional']:\n",
        "        download_a = drive.CreateFile({'id': a[i]})\n",
        "        download_b = drive.CreateFile({'id': b[i]})\n",
        "        download_c = drive.CreateFile({'id': c[i]})\n",
        "    \n",
        "        download_a.GetContentFile(p+'.index')\n",
        "        download_b.GetContentFile(p+'.meta')    \n",
        "        download_c.GetContentFile(p+'.data-00000-of-00001')   \n",
        "        i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ozIxSDxYj72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_sequence()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def begin_chat():\n",
        "    string = \"Bot: Hi!\\nBot: I'm the NLP ChatBot, you can change my personalities by saying 'Change Personality to' and then your personality of choice.\\nBot: There are three to choose from: Professional, Comic, and Friend.\\nBot: You can stop chatting by typing 'Goodbye,' and I can save a log of our chat if you tell me 'Save Chat'\"\n",
        "    log.append(string)\n",
        "    print(string)\n",
        "    return 'friend'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "source": [
        "To change the personality simply type the message \"Change personality to \" followed by the personality you wish to switch to "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def change_personality(p):\n",
        "    if p.lower() not in ['friend', 'comic', 'professional']:\n",
        "        print(\"Sorry, I don't have a personality by that name. My personalities are 'friend', 'comic', and 'professional'\")\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def save_chat():\n",
        "    chat_log = open('Chatlog'+str(datetime.datetime.now())[:19]+'.txt', 'w')\n",
        "    for l in log:\n",
        "        chat_log.write(l+'\\n')\n",
        "    chat_log.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT_DeoHSMw49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def end_chat():\n",
        "    log.append('Bot: Goodbye!')\n",
        "    print('Bot: Goodbye!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_woLwuU3Mk3w",
        "colab_type": "text"
      },
      "source": [
        "To train the chatbot again you simply need to follow down the code cells from section 1 down to section 2.2.3. Theres no need to save or load the seq2seq model since these functions simply save it to the google drive. You can swap the variable \"local\" in the word embedding load and save functions to skip uploading them to the google drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LrbcP_PKap",
        "colab_type": "text"
      },
      "source": [
        "Run the afforementioned functions in 3.1 to 3.4 to define the needed functions, then execute the following code cells in order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69hmYsRvIqRZ",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGqJOU51TWPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer(question):\n",
        "    question = tokenise(question)\n",
        "    question = to_lowercase(question)\n",
        "    question = remove_nonalphabetic(question)\n",
        "    question = list(filter(None, question))\n",
        "    \n",
        "    sequence_data = [question, '_U_']\n",
        "    input_batch, output_batch, target_batch = make_batch([sequence_data], response_indexes)\n",
        "    \n",
        "    \n",
        "    session = tf.Session()\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(session, './'+personality)\n",
        "    prediction = tf.argmax(model, 2)\n",
        "    result = session.run(prediction, feed_dict={\n",
        "                            encoder_input: input_batch,\n",
        "                            decoder_input: output_batch,\n",
        "                            targets: target_batch\n",
        "                        })\n",
        "    return responses[result[0][0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvzZsB7PbYf",
        "colab_type": "code",
        "outputId": "80a0acb5-343a-4a1f-bce2-db7609e7abbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        }
      },
      "source": [
        "chatting = True\n",
        "log = []\n",
        "personality = begin_chat()\n",
        "\n",
        "responses = resp_dict[personality][0]\n",
        "response_indexes = resp_dict[personality][1]\n",
        "\n",
        "while(chatting):\n",
        "    message = input()\n",
        "    log.append('You:' + message)\n",
        "    message = message.lower()\n",
        "    if message.startswith('change personality to'):\n",
        "        p = message.split()[-1]\n",
        "        if change_personality(p):\n",
        "            personality = p\n",
        "            responses = resp_dict[personality][0]\n",
        "            response_indexes = resp_dict[personality][1]\n",
        "    elif message == 'goodbye':\n",
        "        end_chat()\n",
        "        chatting = False\n",
        "    elif message == 'save chat':\n",
        "        save_chat()\n",
        "    elif len(message) == 0:\n",
        "        bot_answer = \"You didn't say anything...\"\n",
        "        log.append('Bot:' + bot_answer)\n",
        "        print(\"Bot:\", bot_answer)\n",
        "    else:\n",
        "        bot_answer = answer(message)\n",
        "        log.append('Bot:' + bot_answer)\n",
        "        print(\"Bot:\", bot_answer)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bot: Hi!\n",
            "Bot: I'm the NLP ChatBot, you can change my personalities by saying 'Change Personality to' and then your personality of choice.\n",
            "Bot: There are three to choose from: Professional, Comic, and Friend.\n",
            "Bot: You can stop chatting by typing 'Goodbye,' and I can save a log of our chat if you tell me 'Save Chat'\n",
            "Hi\n",
            "Bot: Hi!\n",
            "How are you?\n",
            "Bot: I'm doing great, thanks for asking!\n",
            "Do you have any parents?\n",
            "Bot: I haven't met any other bots, but I bet we'd get along.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-97b432c113a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}